# Äá»€ TÃ€I: WORD2VEC VÃ€ DOC2VEC CHO PHÃ‚N LOáº I VÄ‚N Báº¢N TIáº¾NG VIá»†T

## ğŸ“‹ Tá»•ng quan Ä‘á» tÃ i

Äá» tÃ i nÃ y nghiÃªn cá»©u vÃ  Ã¡p dá»¥ng cÃ¡c mÃ´ hÃ¬nh **Word2Vec** vÃ  **Doc2Vec** Ä‘á»ƒ há»c biá»ƒu diá»…n vÄƒn báº£n tiáº¿ng Viá»‡t, sau Ä‘Ã³ sá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n phÃ¢n loáº¡i Ä‘á»ƒ thá»±c hiá»‡n bÃ i toÃ¡n phÃ¢n loáº¡i vÄƒn báº£n.

## ğŸ¯ Má»¥c tiÃªu

1. **NghiÃªn cá»©u lÃ½ thuyáº¿t**: TÃ¬m hiá»ƒu mÃ´ hÃ¬nh Word2Vec, Doc2Vec vÃ  word embedding
2. **CÃ i Ä‘áº·t thá»±c nghiá»‡m**: Triá»ƒn khai cÃ¡c mÃ´ hÃ¬nh biá»ƒu diá»…n vÄƒn báº£n
3. **XÃ¢y dá»±ng dataset**: Thu tháº­p ~500 vÄƒn báº£n tiáº¿ng Viá»‡t vá»›i 4-5 nhÃ£n phÃ¢n loáº¡i
4. **So sÃ¡nh hiá»‡u quáº£**: ÄÃ¡nh giÃ¡ TF-IDF vs Word2Vec vs Doc2Vec
5. **PhÃ¢n loáº¡i vÄƒn báº£n**: Ãp dá»¥ng Rocchio, K-NN, Naive Bayes

## ğŸ§  LÃ½ thuyáº¿t cÆ¡ báº£n

### Word Embedding lÃ  gÃ¬?

**Word Embedding** lÃ  ká»¹ thuáº­t biá»ƒu diá»…n tá»« dÆ°á»›i dáº¡ng vector sá»‘ thá»±c trong khÃ´ng gian nhiá»u chiá»u, giÃºp mÃ¡y tÃ­nh hiá»ƒu Ä‘Æ°á»£c Ã½ nghÄ©a vÃ  má»‘i quan há»‡ giá»¯a cÃ¡c tá»«.

### TF-IDF (Term Frequency - Inverse Document Frequency)

- **NguyÃªn lÃ½**: Äo trá»ng sá»‘ cá»§a tá»« dá»±a trÃªn táº§n suáº¥t xuáº¥t hiá»‡n trong tÃ i liá»‡u vÃ  Ä‘á»™ hiáº¿m trong toÃ n bá»™ corpus
- **CÃ´ng thá»©c**: `TF-IDF(t,d) = TF(t,d) Ã— IDF(t)`
- **Æ¯u Ä‘iá»ƒm**: ÄÆ¡n giáº£n, hiá»‡u quáº£ vá»›i dá»¯ liá»‡u nhá»
- **NhÆ°á»£c Ä‘iá»ƒm**: KhÃ´ng náº¯m báº¯t Ä‘Æ°á»£c ngá»¯ cáº£nh vÃ  Ã½ nghÄ©a cá»§a tá»«

### Word2Vec

- **NguyÃªn lÃ½**: Há»c biá»ƒu diá»…n vector cá»§a tá»« dá»±a trÃªn ngá»¯ cáº£nh xung quanh
- **Kiáº¿n trÃºc**:
  - **CBOW** (Continuous Bag of Words): Dá»± Ä‘oÃ¡n tá»« trung tÃ¢m tá»« ngá»¯ cáº£nh
  - **Skip-gram**: Dá»± Ä‘oÃ¡n ngá»¯ cáº£nh tá»« tá»« trung tÃ¢m
- **Æ¯u Ä‘iá»ƒm**: Náº¯m báº¯t Ä‘Æ°á»£c Ã½ nghÄ©a ngá»¯ nghÄ©a, tá»« tÆ°Æ¡ng tá»± cÃ³ vector gáº§n nhau
- **NhÆ°á»£c Ä‘iá»ƒm**: KhÃ´ng xá»­ lÃ½ Ä‘Æ°á»£c tá»« ngoÃ i vocab, cáº§n dá»¯ liá»‡u lá»›n

### Doc2Vec (Paragraph Vector)

- **NguyÃªn lÃ½**: Má»Ÿ rá»™ng Word2Vec Ä‘á»ƒ há»c biá»ƒu diá»…n toÃ n bá»™ tÃ i liá»‡u
- **Kiáº¿n trÃºc**:
  - **PV-DM** (Distributed Memory): TÆ°Æ¡ng tá»± CBOW nhÆ°ng cÃ³ thÃªm vector tÃ i liá»‡u
  - **PV-DBOW** (Distributed Bag of Words): TÆ°Æ¡ng tá»± Skip-gram cho tÃ i liá»‡u
- **Æ¯u Ä‘iá»ƒm**: Biá»ƒu diá»…n trá»±c tiáº¿p tÃ i liá»‡u, giá»¯ Ä‘Æ°á»£c thÃ´ng tin ngá»¯ cáº£nh
- **NhÆ°á»£c Ä‘iá»ƒm**: Cáº§n nhiá»u dá»¯ liá»‡u, tÃ­nh toÃ¡n phá»©c táº¡p

## ğŸ“Š Cáº¥u trÃºc dá»¯ liá»‡u

```
Dataset: ~500 vÄƒn báº£n tiáº¿ng Viá»‡t
NhÃ£n: 4-5 loáº¡i chá»§ Ä‘á»/thá»ƒ loáº¡i
Chia dá»¯ liá»‡u: 80% train - 20% test (stratified)
```

## ğŸ”¬ Chi tiáº¿t tá»«ng bÆ°á»›c thá»±c nghiá»‡m

### Cell 12: Tiá»n xá»­ lÃ½ dá»¯ liá»‡u

```python
def preprocess(text):
    text = re.sub(f"[{string.punctuation}]", " ", text)
    return text.lower()
```

**Má»¥c Ä‘Ã­ch**:

- Loáº¡i bá» dáº¥u cÃ¢u
- Chuyá»ƒn vá» chá»¯ thÆ°á»ng
- Chuáº©n hÃ³a vÄƒn báº£n cho mÃ´ hÃ¬nh

### Cell 13-15: TF-IDF Vectorization

```python
tfidf_vectorizer = TfidfVectorizer()
tfidf_vectorizer.fit(sentences)
df["tf_idf"] = df["sentence"].apply(lambda x: tfidf_vectorizer.transform([x]).toarray()[0])
```

**NguyÃªn lÃ½ TF-IDF**:

1. **TF (Term Frequency)**: Äáº¿m táº§n suáº¥t tá»« trong tÃ i liá»‡u
2. **IDF (Inverse Document Frequency)**: TÃ­nh Ä‘á»™ hiáº¿m cá»§a tá»« trong corpus
3. **Káº¿t há»£p**: Tá»« quan trá»ng = xuáº¥t hiá»‡n nhiá»u trong tÃ i liá»‡u nhÆ°ng Ã­t trong corpus

### Cell 16-17: Word2Vec Implementation

```python
sentences = [word_tokenize(s) for s in sentences]
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, epochs=50)
```

**Tham sá»‘ quan trá»ng**:

- `vector_size=100`: KÃ­ch thÆ°á»›c vector biá»ƒu diá»…n tá»«
- `window=5`: KÃ­ch thÆ°á»›c cá»­a sá»• ngá»¯ cáº£nh (xem 5 tá»« trÆ°á»›c/sau)
- `min_count=1`: Tá»« pháº£i xuáº¥t hiá»‡n Ã­t nháº¥t 1 láº§n
- `epochs=50`: Sá»‘ láº§n láº·p qua toÃ n bá»™ dá»¯ liá»‡u

**CÃ¡ch táº¡o vector cÃ¢u**:

```python
def sentence_to_vec(sent):
    return np.mean([model.wv[w] for w in word_tokenize(sent)], axis=0)
```

Láº¥y **trung bÃ¬nh** vector cá»§a táº¥t cáº£ tá»« trong cÃ¢u

### Cell 18-19: Doc2Vec Implementation

```python
sentences = [TaggedDocument(words=word_tokenize(row['sentence']), tags=[str(i)])
             for i, row in df.iterrows()]
model = Doc2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, epochs=50)
```

**KhÃ¡c biá»‡t vá»›i Word2Vec**:

- Má»—i tÃ i liá»‡u cÃ³ má»™t **tag** duy nháº¥t
- MÃ´ hÃ¬nh há»c **trá»±c tiáº¿p** vector cho tÃ i liá»‡u
- KhÃ´ng cáº§n trung bÃ¬nh vector tá»«

### Cell 20-21: Chia dá»¯ liá»‡u Train/Test

```python
X_train, X_test, y_train, y_test = train_test_split(
    features, labels, test_size=0.2, stratify=labels, random_state=2025
)
```

**Stratified Split**: Äáº£m báº£o tá»· lá»‡ cÃ¡c nhÃ£n giá»‘ng nhau trong train vÃ  test

### Cell 22: Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i

#### 1. Rocchio Classifier (NearestCentroid)

**NguyÃªn lÃ½**:

- TÃ­nh **centroid** (trung tÃ¢m) cá»§a má»—i class
- PhÃ¢n loáº¡i báº±ng cÃ¡ch tÃ¬m centroid gáº§n nháº¥t
- PhÃ¹ há»£p vá»›i dá»¯ liá»‡u cÃ³ cÃ¡c class tÃ¡ch biá»‡t rÃµ rÃ ng

#### 2. K-Nearest Neighbors (KNN)

**NguyÃªn lÃ½**:

- TÃ¬m **k=10** Ä‘iá»ƒm gáº§n nháº¥t trong khÃ´ng gian feature
- PhÃ¢n loáº¡i theo **Ä‘a sá»‘** cá»§a k neighbors
- KhÃ´ng cáº§n training, lazy learning

#### 3. Naive Bayes

**NguyÃªn lÃ½**:

- Ãp dá»¥ng **Ä‘á»‹nh lÃ½ Bayes** vá»›i giáº£ thiáº¿t Ä‘á»™c láº­p
- `P(class|features) âˆ P(features|class) Ã— P(class)`
- Hiá»‡u quáº£ vá»›i dá»¯ liá»‡u nhá»

### Cell 23-30: ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh

```python
def evaluate(model, X_test, y_test, feature, algorithm):
    y_pred = model.predict(X_test)
    f1 = f1_score(y_test, y_pred, average='micro')
    precision = precision_score(y_test, y_pred, average='micro')
    recall = recall_score(y_test, y_pred, average='micro')
```

**Metrics Ä‘Ã¡nh giÃ¡**:

- **Precision**: Äá»™ chÃ­nh xÃ¡c dá»± Ä‘oÃ¡n
- **Recall**: Kháº£ nÄƒng phÃ¡t hiá»‡n
- **F1-Score**: Trung bÃ¬nh Ä‘iá»u hÃ²a cá»§a Precision vÃ  Recall
- **Confusion Matrix**: Ma tráº­n nháº§m láº«n

## ğŸ“ˆ Káº¿t quáº£ vÃ  phÃ¢n tÃ­ch

### So sÃ¡nh hiá»‡u quáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p

| PhÆ°Æ¡ng phÃ¡p  | Æ¯u Ä‘iá»ƒm                                   | NhÆ°á»£c Ä‘iá»ƒm                            | PhÃ¹ há»£p                      |
| ------------ | ----------------------------------------- | ------------------------------------- | ---------------------------- |
| **TF-IDF**   | ÄÆ¡n giáº£n, nhanh, hiá»‡u quáº£ vá»›i dá»¯ liá»‡u nhá» | KhÃ´ng hiá»ƒu ngá»¯ nghÄ©a                  | Dataset nhá», tá»« vá»±ng á»•n Ä‘á»‹nh |
| **Word2Vec** | Hiá»ƒu ngá»¯ nghÄ©a, tá»« tÆ°Æ¡ng tá»± gáº§n nhau      | Máº¥t thÃ´ng tin thá»© tá»±, cáº§n dá»¯ liá»‡u lá»›n | Dataset lá»›n, cáº§n semantic    |
| **Doc2Vec**  | Biá»ƒu diá»…n trá»±c tiáº¿p tÃ i liá»‡u              | Cáº§n ráº¥t nhiá»u dá»¯ liá»‡u, phá»©c táº¡p       | Dataset ráº¥t lá»›n              |

### NguyÃªn nhÃ¢n TF-IDF hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n

1. **KÃ­ch thÆ°á»›c dá»¯ liá»‡u**: ~500 vÄƒn báº£n quÃ¡ Ã­t cho Word2Vec/Doc2Vec
2. **Cháº¥t lÆ°á»£ng training**: Word2Vec cáº§n hÃ ng triá»‡u tá»« Ä‘á»ƒ há»c tá»‘t
3. **Máº¥t thÃ´ng tin**: Word2Vec láº¥y trung bÃ¬nh â†’ máº¥t ngá»¯ cáº£nh
4. **Overfitting**: Doc2Vec dá»… overfit vá»›i dá»¯ liá»‡u nhá»
